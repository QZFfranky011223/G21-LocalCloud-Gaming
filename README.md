# ‚òÅÔ∏è LocalCloud Gaming: Infraestructura Distribuida de Servidores de Juego
# UNIVERSIDAD MAYOR, REAL Y PONTIFICIA DE SAN FRANCISCO XAVIER DE CHUQUISACA
## FACULTAD DE TECNOLOG√çA

![USFX Logo](https://img.shields.io/badge/USFX-Sistemas-red?style=for-the-badge) 
![Status](https://img.shields.io/badge/Estado-Finalizado-success?style=for-the-badge)
![Docker](https://img.shields.io/badge/Docker-Enabled-blue?style=for-the-badge&logo=docker)

---

# üöÄ Proyecto Final SIS313: LocalCloud Gaming (Infraestructura Distribuida)

**Asignatura:** SIS313: Infraestructura, Plataformas Tecnol√≥gicas y Redes  
**Semestre:** 2/2025  
**Docente:** Ing. Marcelo Quispe Ortega  

---

## üë• Miembros del Equipo (Grupo G-21)

| Nombre Completo | Rol en el Proyecto | Contacto (GitHub) |
| :--- | :--- | :--- |
| **Huanca Coronado Oscar Santiago** | Arquitecto de Infraestructura y Redes | [@ssantiagoxx](https://github.com/ssantiagoxx) |
| **Mollinedo Siles Renzo Sebastian** | Ingeniero de Automatizaci√≥n (DevOps) | [@SoKierkegaard](https://github.com/SoKierkegaard) |
| **Quispe Zarate Franky** | Administrador de Servidores y Almacenamiento | [@QZFfranky011223](https://github.com/QZFfranky011223) |
| **Vargas Alarc√≥n Brayan Mario** | Especialista en Seguridad y Monitoreo | [@TheBranx](https://github.com/) |

---

## üéØ I. Objetivo del Proyecto

**Objetivo:** Dise√±ar e implementar una infraestructura de TI distribuida y virtualizada para alojar servicios de videojuegos (Luanti y Minecraft) en contenedores, garantizando la redundancia de datos mediante RAID 1, seguridad perimetral y monitoreo centralizado en tiempo real.

## üí° II. Justificaci√≥n e Importancia

**Justificaci√≥n:**  
Este proyecto es relevante porque simula un entorno de producci√≥n real donde la continuidad del servicio es cr√≠tica. Resuelve problemas de **P√©rdida de Datos (T2)** mediante la implementaci√≥n de RAID 1 y estrategias de backup automatizado (Regla 3-2-1). Adem√°s, aborda la **Gesti√≥n de Redes (T3)** mediante un servidor DNS local (Pi-hole) y mejora la **Seguridad (T5)** mediante segmentaci√≥n y proxies inversos, alej√°ndose de las configuraciones monol√≠ticas vulnerables a fallos √∫nicos.

## üõ†Ô∏è III. Tecnolog√≠as y Conceptos Implementados

### 3.1. Tecnolog√≠as Clave
*   **Docker & Docker Compose:** Orquestaci√≥n de contenedores para los servicios de juego (PaperMC, Luanti) y monitoreo, asegurando aislamiento y f√°cil despliegue.
*   **mdadm (Linux RAID):** Herramienta para la gesti√≥n de RAID por software, utilizada para crear un arreglo RAID 1 (Espejo) en el nodo de almacenamiento.
*   **Prometheus & Grafana:** Sistema de recolecci√≥n de m√©tricas y visualizaci√≥n. Prometheus extrae datos de los *node-exporters* y Grafana los presenta en dashboards (CPU, RAM, Red).
*   **Bash Scripting & Cron:** Automatizaci√≥n de tareas de mantenimiento, men√∫s de gesti√≥n (`menu_servidor.sh`) y copias de seguridad autom√°ticas v√≠a SCP.
*   **Pi-hole:** Servidor DNS local para la resoluci√≥n de nombres de dominio internos (ej. `dashboard.juego.lan`) y bloqueo de tr√°fico no deseado.
*   **Nginx Proxy Manager:** Gesti√≥n de Proxy Inverso para forzar conexiones SSL/TLS seguras hacia los paneles de administraci√≥n.

### 3.2. Conceptos de la Asignatura Puestos en Pr√°ctica (T1 - T6)
- ‚úÖ **Alta Disponibilidad (T2) y Tolerancia a Fallos:** Implementaci√≥n de RAID 1 en el nodo de Storage y recuperaci√≥n de servicios mediante backups externos.
- ‚úÖ **Seguridad y Hardening (T5):** Uso de Firewalls (UFW), llaves SSH para transferencias sin contrase√±a y simulaci√≥n de ataques DoS con `hping3`.
- ‚úÖ **Automatizaci√≥n y Gesti√≥n (T6):** Scripts de Bash para la gesti√≥n de contenedores y automatizaci√≥n de backups con retenci√≥n de 14 d√≠as.
- ‚úÖ **Balanceo de Carga/Proxy (T3/T4):** Implementaci√≥n de Nginx como punto de entrada seguro (HTTPS).
- ‚úÖ **Monitoreo (T4/T1):** Despliegue de agentes Node Exporter en 5 nodos y centralizaci√≥n de alertas en VM-Monitor.
- ‚úÖ **Networking Avanzado (T3):** Configuraci√≥n de DNS interno, IPs est√°ticas y enrutamiento en red local.

## üåê IV. Dise√±o de la Infraestructura y Topolog√≠a

### 4.1. Dise√±o Esquem√°tico
La infraestructura opera en la red `192.168.0.0/24` con 5 nodos especializados.

| VM/Host | Rol | IP Est√°tica | Servicios Principales | SO |
| :--- | :--- | :--- | :--- | :--- |
| **VM 1** | Storage / RAID | `192.168.0.201` | RAID 1 (mdadm), SSH Server | Ubuntu 24.04 |
| **VM 2** | Compute (Juegos) | `192.168.0.202` | Docker (Luanti, Minecraft), Scripts | Ubuntu 24.04 |
| **VM 3** | Monitor / Gateway | `192.168.0.203` | Grafana, Prometheus, Nginx | Ubuntu 24.04 |
| **VM 4** | Admin / Attacker | `192.168.0.204` | hping3, Cliente SSH | Ubuntu 24.04 |
| **VM 5** | Infra DNS | `192.168.0.205` | Pi-hole (Docker) | Ubuntu 24.04 |

### 4.2. Estrategia Adoptada
*   **Estrategia de Separaci√≥n de Roles:** Se decidi√≥ desacoplar el c√≥mputo (VM 2) del almacenamiento (VM 1). Esto permite que, si el servidor de juegos se satura o corrompe, los respaldos permanezcan seguros e intactos en un nodo f√≠sico/l√≥gico distinto protegido por RAID.
*   **Estrategia de Seguridad 3-2-1:** Los backups se generan localmente, se comprimen y se env√≠an a un almacenamiento remoto (VM Storage), garantizando que existan al menos dos copias de los datos en diferentes medios.

## üìã V. Gu√≠a de Implementaci√≥n y Puesta en Marcha

### 5.1. Pre-requisitos
*   Hypervisor (VirtualBox/VMware) configurado en modo "Adaptador Puente".
*   5 M√°quinas Virtuales con Ubuntu Server 24.04 LTS instalado.
*   Acceso a internet en las VMs para la instalaci√≥n inicial de paquetes.

### 5.2. Despliegue

**1. Configuraci√≥n de Red (Netplan):**
Editar `/etc/netplan/00-installer-config.yaml` en cada VM para asignar las IPs est√°ticas (201 a 205) y establecer el DNS server a `192.168.0.205` (VM 5).

**2. Despliegue de Servicios (Docker):**
En VM 2 (Compute) y VM 3 (Monitor), clonar el repositorio y ejecutar:
```
sudo mdadm --create --verbose /dev/md0 --level=1 --raid-devices=2 /dev/sdb /dev/sdc
sudo mkfs.ext4 /dev/md0
sudo mount /dev/md0 /var/backups/clientes_juegos
```
### 5.3. Ficheros de Configuraci√≥n Clave
*   `/nodo-compute/docker-compose.yml`: Define los servicios de Minecraft (PaperMC) y Luanti, limitando recursos (RAM/CPU).
*   `/nodo-compute/scripts/menu_servidor.sh`: Script interactivo para administraci√≥n y backups manuales.
*   `/nodo-monitor/prometheus.yml`: Configuraci√≥n de scraping para recolectar m√©tricas de las IPs 192.168.0.201 a 205.
*   `/etc/crontab` (en VM 2): Programaci√≥n de la tarea de respaldo a las 03:00 AM.

## ‚ö†Ô∏è VI. Pruebas y Validaci√≥n

| Prueba Realizada | Resultado Esperado | Resultado Obtenido |
| :--- | :--- | :--- |
| **Simulaci√≥n de Ataque DoS** (hping3 desde VM 4) | El uso de CPU en VM 2 debe subir dr√°sticamente y Grafana debe registrar el pico. | **[OK]** Grafana mostr√≥ uso de CPU > 90% y alerta visual. |
| **Validaci√≥n de Backup Autom√°tico** | El archivo `.tar.gz` debe aparecer en la carpeta RAID de la VM 1 sin intervenci√≥n manual. | **[OK]** Archivo recibido correctamente v√≠a SCP. |
| **Resoluci√≥n DNS Interna** | Ping a `dashboard.juego.lan` debe resolver a `192.168.0.203`. | **[OK]** Pi-hole resolvi√≥ el dominio correctamente. |
| **Acceso Seguro Web** | Acceso al panel de control v√≠a HTTP debe redirigir o bloquearse, permitiendo solo HTTPS. | **[OK]** Nginx Proxy Manager gestion√≥ el certificado SSL. |

## üìö VII. Conclusiones y Lecciones Aprendidas

El proyecto **LocalCloud Gaming** demostr√≥ la viabilidad de utilizar tecnolog√≠as de contenedores y virtualizaci√≥n para crear servicios robustos de entretenimiento.

*   **Logros:** Se logr√≥ una integraci√≥n exitosa entre servicios dispares (Juegos, DNS, Monitoreo) utilizando una red interna est√°tica. La implementaci√≥n de RAID 1 y la automatizaci√≥n de backups aseguran la integridad de los datos de los usuarios, un activo cr√≠tico en servidores de juegos.
*   **Desaf√≠os Superados:** La configuraci√≥n de la comunicaci√≥n segura entre nodos (SSH Keys) y la correcta configuraci√≥n de los targets en Prometheus requirieron un ajuste fino de los firewalls (UFW) para permitir el tr√°fico en puertos espec√≠ficos (9100, 3000, 22).
*   **Lecci√≥n Aprendida:** La observabilidad no es opcional. Durante las pruebas de estr√©s, sin Grafana hubiera sido dif√≠cil identificar qu√© recurso (CPU vs RAM) estaba siendo el cuello de botella.

---
¬© 2025 Facultad de Tecnolog√≠a - USFX